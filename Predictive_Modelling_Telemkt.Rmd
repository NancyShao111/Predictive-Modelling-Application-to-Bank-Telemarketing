---
title: "Final Project"
author: "Lanxiang Shao"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(corrplot)
library(dplyr)
library(ggplot2)
library(caret)
library(naivebayes)

dta_bank <- read.csv("Bank Case.csv")
```

# Basic Explanatory Analysis

## 1. 
The data is loaded and named above.

## 2.
#### In one sentence, describe variables in each column paying special attention to
#### a. Type of variable (categorical/numerical) and what are the units (for the numerical only)

##### age: numeric, the unit is year, showing how old they are.
##### job: categorical, the occupation of these people.
##### marital: categorical, if they are married or not.
##### education: categorical, education attainment situation.
##### default: categorical, any credit default records.
##### housing: categorical, have house or not.
##### loan: categorical, have loan or not.
##### contact: categorical, contacting methods.
##### month: categorical, which month.
##### day_of_week: categorical, which day of the week.
##### duration: numeric, the unit is second, the call duration.
##### y: categorical, whether join the bank or not.


#### b. For the ones that are numerical study whether they have outliers. There is no definition for what an outlier so we can define 
#### an outlier as any observation with a value that is more than 4 times its standard deviation.

#### Conclusion: variables age and duration both have outliers. See the code below to prove.
```{r}
have_outliers = function(data_list){
  mean = mean(data_list)
  sd = sd(data_list)
  outlier = mean + 4 * sd | mean - 4 * sd
  if(sum(outlier > 0)){
    print('have outliers')
    return(TRUE)
    }
    else {print('no outliers')
      return(FALSE)
    }}
```  

```{r}
have_outliers(dta_bank$age)
```

```{r}
have_outliers(dta_bank$duration)
```

*The variable that will focus our study is y and it indicates whether the household actually decided to join the bank. We will*
*see how we can use the predictive modeling techniques seen in class to improve the efficiency making marketing phone calls.*

## 3.
### Create a corr-plot
```{r}
job_2 = as.numeric(as.factor(dta_bank$job))
marital_2 = as.numeric(as.factor(dta_bank$marital))
education_2 = as.numeric(as.factor(dta_bank$education))
default_2 = as.numeric(as.factor(dta_bank$default))
housing_2 = as.numeric(as.factor(dta_bank$housing))
loan_2 = as.numeric(as.factor(dta_bank$loan))
contact_2 = as.numeric(as.factor(dta_bank$contact))
month_2 = as.numeric(as.factor(dta_bank$month))
dow_2 = as.numeric(as.factor(dta_bank$day_of_week))
y_2 = as.numeric(as.factor(dta_bank$y))

dta_bank_2 = data.frame(dta_bank$age,job_2,marital_2,education_2,default_2,housing_2,loan_2,
contact_2,month_2, dow_2,dta_bank$duration, y_2)
M <- cor(dta_bank_2)
corrplot(M, method = "circle")
```

## 4.
```{r}
dta_bank$y = as.numeric(dta_bank$y)
#(1 for no, 2 for yes)

#Run the following command lm(y~.,data=dta_bank)
reg1 = lm(y~.,data=dta_bank)
summary(reg1)
```
### a.
y = 1.028e+00 + 7.978e-04 * age + -2.596e-02 * jobblue-collar + -2.669e-02 * jobentrepreneur + ... (too many variables, omit some) + 2.683e-03 * day_of_weekthu + 6.653e-03 * day_of_weektue + 7.407e-03 * day_of_weekwed + 4.825e-04 * duration

### b.
#### i. Best time to perform telemarketing tasks?
Best month: March.
Best day: Wednesday.
   
#### ii. Best income groups?
Best income groups:student.

#### iii. Potential concerns of omitted variable Bias
Some variables like the number of the bank branches and the general economic situation of that year will rise the OVB concerns since they are related to other Xs (the variables) and they are also determints of y.


# Predictive Modeling and Tuning

## 1. 
*Why we always divide the data set into training and test sets?*

Training dataset is the sample of data that we use to fit the model. The actual dataset that we use to train the model. The model sees and learns from this data. The validation set is used to evaluate a given model. We use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never learns from this. We use the validation data results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly. Test Dataset is the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.

## 2.
*From the point of view of the firm and given that we are running a predictive exercise, is there any variable that should not be* *included as X? If yes, please drop it.*

I decide to drop the variable duration since the call duration is totally decided by the customers themselves. The bank cannot control the duration. Also, usually at the end of the call, the bank employers would already known the customers' choice (whether join the bank or not).
```{r}
dta_bank = dta_bank %>% select(-c(duration))
```

## 3.
*Explain the problems of overfitting and underfitting*

Overfitting happens when a model models the training data too well. Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. Underfitting means a model that can neither model the training data nor generalize to new data. An underfit model will be obvious since it will have poor performance on the training data.

## 4.
*Explain the meaning of the no free lunch theorem*

In the no free lunch theorem, no matter that kind of problems we are going to solve, the computational cost of finding the suitable solutions is the same for each solution method. No solution can save the time, money or any other cost for you.

## 5.
```{r}
lm1 = lm(y~age+factor(month), data = dta_bank)
summary(lm1)
```
y = 1.1960515 +  0.0002152 * age + -0.0989919 * factor(month)aug + 0.2827789 * factor(month)dec + ... + 0.2331616 * factor(month)oct + 0.2436841 * factor(month)sep

```{r}
age2 = (dta_bank$age)^2
age3 = (dta_bank$age)^3
lm2 = lm(y~age+age2+age3+factor(month),data = dta_bank)
summary(lm2)
```
y =  1.841e+00 + -3.610e-02 * age +  5.761e-04 * age^2 + -2.332e-06 * age^3 + ... + 2.033e-01 * factor(month)oct + 2.139e-01 * factor(month)sep

```{r}
lm3 = lm(y~., data = dta_bank)
summary(lm3)
```
y = 1.167e+00 + 8.316e-04 * age  - 2.293e-02 * jobblue-collar - 2.282e-02 * jobentrepreneur + ... +  1.090e-02 * day_of_weektue + 1.404e-02 * day_of_weekwed

```{r}
lm4 = lm(y~.^2, data = dta_bank)
summary(lm4)
```
y =  9.962e-01 +  1.821e-03 * age + 7.550e-03 * jobblue-collar + -9.690e-02 * jobentrepreneur + ... +  3.215e-02 * jobself-employed:defaultunknown                                                                

### a.
The lm4 overfits more since it puts too much infomation in the model.

### b.
the lm1 is underfitting more since the variables it uses are not that enough.

### c.
*Is the model that fits the training data the best one that has the best predictive power?*
```{r, warning = FALSE}
N = nrow(dta_bank)
training_index = sample(N)[1:round(0.6*N)]
dta_bank_training = dta_bank[training_index,]
dta_bank_validating = dta_bank[-training_index, ][1:round(0.2*N),]
dta_bank_test = dta_bank[-training_index, ][(round(0.2*N)+1):round(0.4*N),]

data_new = dta_bank_test[1:11]

pre_1 = predict(lm1, data_new)
cor(dta_bank_test$y, pre_1)

pre_3 = predict(lm3, data_new)
cor(dta_bank_test$y, pre_3)

pre_4 = predict(lm4, data_new)
cor(dta_bank_test$y, pre_4)
```

```{r}
#age^2 (age2) and age^3 (age3) are added in this regression, so make a new dataset with variable age2 and age3.
dta_bank_lm2 = dta_bank %>% mutate(age2,age3)
N_lm2 = nrow(dta_bank_lm2)
training_index_lm2 = sample(N_lm2)[1:round(0.6*N_lm2)]
dta_bank_training_lm2 = dta_bank_lm2[training_index_lm2,]
dta_bank_validating_lm2 = dta_bank_lm2[-training_index_lm2, ][1:round(0.2*N_lm2),]
dta_bank_test_lm2 = dta_bank_lm2[-training_index_lm2, ][(round(0.2*N_lm2)+1):round(0.4*N_lm2),]

data_new_lm2 = dta_bank_test_lm2

pre_2 = predict(lm2, data_new_lm2)
cor(dta_bank_test_lm2$y, pre_2)
```
No, because it may fit into some unrelated and unimportant variables in the training data. That will not help our predictions a lot and that model does not have the best predictive power. The model lm4 has the highest accuracy, but lm4 is overfitting compared to lm3. 

### d.
*Can we use a confusion matrix to analyze the problems a problem of underfitting?*
```{r}
#probability that the answer will be 'no'
print(mean(dta_bank$y == 1))
```
Yes. For both the accuracy of the training dataset and the test dataset, if two accuracy rates are close or even lower than the probability of saying 'no' to all the observations in our original dta_bank data (that is close or lower than 0.887), then it means the model probably has the problem of underfitting.

### e.
We should use the training dataset of lm3 to run these regressions. 
After running these regressions, we can use the validating dataset to see if we meet the problem of overfitting or underfitting. Then, we can use the test dataset to evaluate the predictive power of the models. If we cannot get a good result, we should start over, like adjusting the dataset division.


# Improving the predictive power

## 1.
*Make a visualization to inspect the relationship between the Y and each of the X that you have included in the regressions above.*
*Does it look linear?*

They do not look linear. Predicted y is numeric values ranging from 1(no)-2(yes) instead of only 1 and 2. 
```{r}
#pre_3 is the predicted y value
pre3 = as.data.frame(pre_3)
data_new$pre_3 = pre3$pre_3
pairs(data_new[,-11])
```

## 2.
*Use the other predictive methods seen in class (like NB classifiers or KNN) to check if you can improve the performance.*

```{r}
#normalize function
Normalize = function(x){return ((x - min(x)) / (max(x) - min(x)))}

#prepare suitable dataset, drop variable duration
dta_bank_2 = dta_bank_2 %>% select(-c(dta_bank.duration))

#renaming y as a factor with proper labels
dta_bank_2$y_2 = factor(x = dta_bank_2$y_2,
                      levels = c("1", "2"),
                      labels = c("NO", "YES"))
#build labels
Label = dta_bank_2[,11]

#create training, validate and test data
num = nrow(dta_bank_2)
training_index_num = sample(num)[1:round(0.6*num)]

dta_bank_training_num = dta_bank_2[training_index,]
dta_bank_validating_num = dta_bank_2[-training_index, ][1:round(0.2*num),]
dta_bank_test_num = dta_bank_2[-training_index, ][(round(0.2*num)+1):round(0.4*num),]

#create labels for data
training_labels_num = Label[training_index]
validating_labels_num = Label[-training_index][1:round(0.2*num)]
test_labels_num = Label[-training_index][(round(0.2*num)+1):round(0.4*num)]

#NaiveBayes
NBclassifier  = e1071::naiveBayes(y_2~., data = dta_bank_validating_num)
NB_prediction = predict(NBclassifier, newdata = dta_bank_validating_num[, -11], type = 'class')
NB_conf_mat  = gmodels::CrossTable(x = validating_labels_num, 
                                      y = NB_prediction,
                                      prop.chisq = T)
confusionMatrix(NB_prediction,validating_labels_num)
```

```{r}
#KNN
#normalize dataset
dta_bank_N = as.data.frame(lapply(dta_bank_2[,1:10], Normalize))

#create training, validate and test data
num = nrow(dta_bank_N)
training_index_num = sample(num)[1:round(0.6*num)]

dta_bank_training_num = dta_bank_N[training_index,]
dta_bank_validating_num = dta_bank_N[-training_index, ][1:round(0.2*num),]
dta_bank_test_num = dta_bank_N[-training_index, ][(round(0.2*num)+1):round(0.4*num),]

#create labels for data
training_labels_num = Label[training_index]
validating_labels_num = Label[-training_index][1:round(0.2*num)]
test_labels_num = Label[-training_index][(round(0.2*num)+1):round(0.4*num)]
#training model
KNN_prediction = class::knn(train = dta_bank_training_num[,-11], 
                            cl    = training_labels_num,
                            test  = dta_bank_validating_num[,-11],
                            k     = 5)

#evaluating performance
KNN_conf_mat  = gmodels::CrossTable(x = validating_labels_num, 
                                    y = KNN_prediction,
                                    prop.chisq = T)

confusionMatrix(data = KNN_prediction, reference = validating_labels_num)
```

## 3.
*Do they make it better? Worse?*

They make it better since the accuracy rates of both accurary NB classifiers and KNN are around 95%, better than the previous lm models.


# Causal Questions

## 1.
*When we study causality we always focus on the parameters multiplying the X variables instead of the predictive capacity of the model.* *We then give a causal interpretation to the estimated coefficients.*

### a. 
*Explain when in marketing is preferable a causal analysis to a predictive analysis.*

In a causal analysis, the independent variables are considered to be causes of the dependent variable. It is pretty clear to see the role of causes and effects. We look at our evidence and draw the conclusion that X causes Y. It is easilly understood. In marketing, sometimes we only want to know the correlation relationship, like they are positively related or negatively related. The degree or the quantity of the future effects are hard to predict accurately. 

### b.
*In the context of a linear regression, explain the concepts of a biased estimated.*

The model does not get the best linear unbiased estimator (BLUE). Bias means that the expected value of the estimator is not equal to the population parameter. Intuitively in a regression analysis, this would mean that the estimate of one of the parameters is too high or too low. OVB usually happen under this kind of situation.

## 2.
*Which of the variables could be interesting to analyze from a causal point of view. Give examples.*

In dataset dta_bank, variables job and marital will be interesting to analyze from a casual point of view. Specifically speaking, the bank just need to know which jobs are more willing to join the bank and which jobs are negatively related to join the bank. The bank just needs roughly to know the positive and negative relationship. Also, people's jobs usually change over time. If people's job changes in future, the current job is hard to support the prediction for the future judgement. This is the same for marital. People's marital status also changes over time. This is a personal choice which is cannot controled by the bank. Therefore, just knowing the rough positive or negative relationship and the coefficient is enough. 

### 3.
*For those variables what would be the potential omitted variables problem?*

Omitted variables are related to the Xs and also are determinant of Y. For the variable job, OVB will arise if we do not conclude the working years. Usually, people working in a short time cannot accumulate much wealth, which discourage them from joining the bank. For instance, they do not have much money to deposit or buy the bank financial products. For the variable marital, we'd better include variables like the number of the children, the size of the household, or if their companion joins the bank. Couples with children are more willing to join the bank, since these families seek for a long-term insurance and would like to consider for their children (like children's future college spending). Joining the bank would be a good choice for them to manage their money. Also, if your husband/wife joinds the bank, you will be more likely to join the bank since you can easily see the benefits of joining the bank.
